{"cells":[{"cell_type":"markdown","metadata":{"id":"hOm9RWv4g47K"},"source":["# Image similarity search\n","\n","指定された画像と似ている画像を検索する。\n","\n","Step 1. Define AutoEncoder\n","* 画像のencoder として AutoEncoder を定義する。\n","\n","Step 2. Lightning Module\n","* encoder を pytorchlightning 用に定義する。\n","\n","Step 3. Training Encoder\n","* encoder 訓練する。\n","* 訓練済みモデルをファイルに保存する。\n","\n","Step 4. Embedding\n","* 検索対象画像と検索画像を特徴ベクトルへ変換する。\n","* 特徴ベクトルをファイルに保存する。\n","\n","Step 5. Faiss indexes\n","* Step 4 で作成した検索対象画像の特徴ベクトルを検索するためのインデックスを生成する。\n","* インデックスをファイルに保存する。\n","\n","Step 6. Image similarity search\n","* Step 4 で作成した検索画像の特徴ベクトルを使用して、Step 5 のインデックスを検索する。\n","\n","-----\n","\n","pytorch:\n","* https://pytorch.org/\n","\n","pytorchligtning:\n","* https://www.pytorchlightning.ai/\n","\n","faiss:\n","* https://github.com/facebookresearch/faiss\n","* https://faiss.ai/\n","\n","Cifar10:\n","* https://pytorch.org/vision/stable/datasets.html\n","\n","AutoEndoder\n","* https://qiita.com/MuAuan/items/a062d0c245c8f4836399"]},{"cell_type":"markdown","metadata":{"id":"N9vMovNyg47O"},"source":["## Setup for Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoudiZC7g47O"},"outputs":[],"source":["# notebook runtime\n","import sys\n","\n","runtime = 'local'\n","if 'google.colab' in sys.modules:\n","    runtime = 'colab'\n","elif _dh == ['/kaggle/working']:\n","    runtime = 'kaggle'\n","runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDHWansfg47O"},"outputs":[],"source":["if runtime == 'colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJEPyetZg47O"},"outputs":[],"source":["if runtime == 'colab':\n","    home_path = '/content/drive/MyDrive/image_similarity_search'\n","else:\n","    home_path = '/home/jovyan/image_similarity_search'\n","\n","nbs_path = f'{home_path}/nbs'\n","datasets_path = f'{home_path}/datasets'\n","models_path = f'{home_path}/models'\n","figs_path = f'{home_path}/figs'\n","logs_path = f'{home_path}/logs'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrnL_u7Xg47O"},"outputs":[],"source":["%cd {nbs_path}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98YxweSbg47P"},"outputs":[],"source":["!pip install -q pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUSUpdJcg47P"},"outputs":[],"source":["!pip install -q faiss-gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPUfAgrZg47P"},"outputs":[],"source":["!pip install -q nb-clean"]},{"cell_type":"markdown","metadata":{"id":"cHXm7fmdg47P"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YArA3L4xg47P"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","import matplotlib.pyplot as plt\n","import pickle\n","import shutil\n","from PIL import Image\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WB3BwD4Yg47P"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchsummary import summary\n","\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeDi_1hQg47P"},"outputs":[],"source":["import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EiKQO3Qg47P"},"outputs":[],"source":["AVAIL_GPUS = min(1, torch.cuda.device_count())\n","BATCH_SIZE = 256 if AVAIL_GPUS else 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5VLmZOCg47Q"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_2ZOaGvg47Q"},"outputs":[],"source":["pl.seed_everything(42)"]},{"cell_type":"markdown","metadata":{"id":"YYB_wXWFg47Q"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrRMWrp2g47Q"},"outputs":[],"source":["from torchvision.datasets import CIFAR10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0k2bA34g47Q"},"outputs":[],"source":["dataset_name = 'cifar10'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3LT7UJUg47Q"},"outputs":[],"source":["def clear_datasets():\n","    path_cifar_10_batches_py = os.path.join(datasets_path, 'cifar-10-batches-py')\n","    if os.path.exists(path_cifar_10_batches_py):\n","        shutil.rmtree(path_cifar_10_batches_py)\n","\n","    path_cifar_10_python = os.path.join(datasets_path, 'cifar-10-python.tar.gz')\n","    if os.path.exists(path_cifar_10_python):\n","        os.remove(path_cifar_10_python)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8sAvIzbg47Q"},"outputs":[],"source":["#clear_datasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fi_8R1Pzg47Q"},"outputs":[],"source":["def cifar10_normalization():\n","    normalize = transforms.Normalize(\n","        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n","        std=[x / 255.0 for x in [63.0, 62.1, 66.7]],\n","    )\n","    return normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hNFjCUNg47Q"},"outputs":[],"source":["def train_transforms():\n","    return torchvision.transforms.Compose([\n","        torchvision.transforms.RandomCrop(32, padding=4),\n","        torchvision.transforms.RandomHorizontalFlip(),\n","        torchvision.transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        #cifar10_normalization(),\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPIUMp_Gg47Q"},"outputs":[],"source":["def test_transforms():\n","    return torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        #cifar10_normalization(),\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJo7IZnUg47R"},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","\n","    def __init__(self, data_dir: str, batch_size: int=32):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.num_workers = int(os.cpu_count() / 2)\n","        self.train_transform = train_transforms()\n","        self.test_transform = test_transforms()\n","        self.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","        self.num_classes = len(self.classes)\n","        self.dims = (3, 32, 32) # channels, width, height\n","\n","    def prepare_data(self):\n","        # download\n","        CIFAR10(self.data_dir, train=True, download=True)\n","        CIFAR10(self.data_dir, train=False, download=True)\n","\n","    def setup(self, stage=None): #train, val, testデータ分割\n","        # Assign train/val datasets for use in dataloaders\n","        datasets = CIFAR10(self.data_dir, train=True, transform=self.train_transform)\n","        n_train = int(len(datasets) * 0.8)\n","        n_val = len(datasets) - n_train\n","        self.ds_train, self.ds_val = torch.utils.data.random_split(datasets, [n_train, n_val])\n","        self.ds_test = CIFAR10(self.data_dir, train=False, transform=self.test_transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.ds_train, shuffle=True, drop_last=True, batch_size=self.batch_size, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.ds_val, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers)\n"," \n","    def test_dataloader(self):\n","        return DataLoader(self.ds_test, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81emp6KNg47R"},"outputs":[],"source":["datamodule = DataModule(datasets_path, BATCH_SIZE)\n","datamodule.prepare_data()\n","datamodule.setup()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcp_INb0g47R"},"outputs":[],"source":["# check data size\n","train_dataloader = iter(datamodule.train_dataloader())\n","images, labels = next(train_dataloader)\n","images.shape, labels.shape"]},{"cell_type":"markdown","metadata":{"id":"gp2TGhc9g47R"},"source":["## Step 1. Define AutoEncoder\n","\n","画像の特徴ベクトルを抽出するためAutoEncoder を定義する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wn8MTOueg47R"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 256, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256)\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzDZJqm-g47R"},"outputs":[],"source":["# check data size\n","m = Encoder()\n","m(torch.randn(32, 3, 32, 32)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfuApdn9g47R"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=256, out_channels=16,\n","                                 kernel_size=2, stride=2, padding=0),\n","            nn.ConvTranspose2d(in_channels=16, out_channels=3,\n","                                 kernel_size=2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_D-TTmNg47R"},"outputs":[],"source":["m = Decoder()\n","m(torch.randn(32, 256, 8, 8)).shape"]},{"cell_type":"markdown","metadata":{"id":"L5K3U2kmg47S"},"source":["## Step 2. Lightning Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"417kW9bkg47S"},"outputs":[],"source":["encoder_name = 'autoencoder'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b-p4Rwpg47S"},"outputs":[],"source":["class LitAutoEncoder(pl.LightningModule):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n","\n","    def training_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"train\")\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"val\")\n","\n","    def test_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"test\")\n","    \n","    def _common_step(self, batch, batch_idx, stage: str):\n","        x, y = batch\n","        loss = F.mse_loss(x, self(x))\n","        self.log(f\"{stage}_loss\", loss, on_step=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VLWDJ-4g47S"},"outputs":[],"source":["# model\n","encoder = LitAutoEncoder()\n","encoder = encoder.to(device)  #for gpu\n","summary(encoder.encoder, (3, 32, 32))\n","summary(encoder.decoder, (256, 8, 8))\n","summary(encoder, (3, 32, 32))\n","print(encoder)"]},{"cell_type":"markdown","metadata":{"id":"2O4gp7L-g47S"},"source":["## Step 3. Training Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GqZv-CDg47S"},"outputs":[],"source":["path_encoder = f'{models_path}/{dataset_name}_{encoder_name}.ckpt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YneMoB-g47S"},"outputs":[],"source":["def clear_encoder():\n","    if os.path.exists(path_encoder):\n","        os.remove(path_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BU25h1dCg47S"},"outputs":[],"source":["clear_encoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-wilqp9g47S"},"outputs":[],"source":["callbacks = [EarlyStopping(monitor=\"val_loss\")]\n","trainer = pl.Trainer(gpus=1, callbacks=callbacks,\n","                logger=TensorBoardLogger(f'{logs_path}/lightning_logs/', name='f{dataset_name}_{encoder_name}'))\n","\n","if not os.path.exists(path_encoder):\n","    #trainer = pl.Trainer(max_epochs=10, gpus=1)\n","    trainer.fit(encoder, datamodule)\n","    trainer.save_checkpoint(path_encoder)\n","else:\n","    encoder = encoder.load_from_checkpoint(path_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzS_9UDVg47T"},"outputs":[],"source":["encoder = encoder.to(device)  #for gpu\n","encoder.freeze()\n","encoder.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79b3k8dig47T"},"outputs":[],"source":["# Test\n","# 'test_loss': 0.003598422510549426\n","results = trainer.test(encoder, datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V34vuOczg47T"},"outputs":[],"source":["# functions to show an image\n","def imshow(img, file=None, title=None):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.detach().numpy()\n","    plt.figure(figsize=(20, 10))\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    if title:\n","        plt.title(title)\n","    if file:\n","        plt.savefig(file + '.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLToXFaYg47T"},"outputs":[],"source":["# Original train images\n","dataiter = iter(datamodule.train_dataloader())\n","images, labels = dataiter.next()\n","imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_original', title='train images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iODdOzEqg47T"},"outputs":[],"source":["# Decoded train images\n","images_hat = encoder(images)\n","imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_decoded', title='train decoded images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOzljrdcg47T"},"outputs":[],"source":["# Original test images\n","dataiter = iter(datamodule.test_dataloader())\n","images, labels = dataiter.next()\n","imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_original', title='test images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATiNoojdg47T"},"outputs":[],"source":["images_hat = encoder(images)\n","imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_decoded', title='test decoded images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"markdown","metadata":{"id":"hTSnQxl_g47T"},"source":["## Step 4. Embedding\n","\n","検索対象画像と検索画像を特徴ベクトルへ変換する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0cziyGXg47T"},"outputs":[],"source":["path_embeded_train = f'{models_path}/{dataset_name}_{encoder_name}_embeded_train.pickle'\n","path_embeded_test = f'{models_path}/{dataset_name}_{encoder_name}_embeded_test.pickle'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGJGpwo5g47U"},"outputs":[],"source":["def clear_embedding():\n","    if os.path.exists(path_embeded_train):\n","        os.remove(path_embeded_train)\n","    if os.path.exists(path_embeded_test):\n","        os.remove(path_embeded_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WB5AcNCcg47U"},"outputs":[],"source":["clear_embedding()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdQRBDaNg47U"},"outputs":[],"source":["train_dataset = CIFAR10(datasets_path, train=True, download=True)\n","test_dataset = CIFAR10(datasets_path, train=False, download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuA1I6KPg47U"},"outputs":[],"source":["len(train_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrUg7moWg47U"},"outputs":[],"source":["# with preprocess(without data augumentation)\n","train_dataloader = DataLoader(CIFAR10(datasets_path, train=True, download=True, transform=test_transforms()),\n","                               shuffle=False, batch_size=32, num_workers=0)\n","test_dataloader = DataLoader(CIFAR10(datasets_path, train=False, download=True, transform=test_transforms()),\n","                               shuffle=False, batch_size=32, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA0bL_GUg47U"},"outputs":[],"source":["len(train_dataloader), len(test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtJoEYi4g47U"},"outputs":[],"source":["def create_embedder_model(autoencoder):\n","    layers = list(autoencoder.encoder.children())\n","    #fv = nn.Sequential(nn.AdaptiveMaxPool2d(output_size=1))\n","    fv = nn.Sequential(nn.Flatten(), nn.Linear(in_features=256*8*8, out_features=256))\n","    model = nn.Sequential(*layers, *fv)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3t5fIGvg47U"},"outputs":[],"source":["embedder = create_embedder_model(encoder)\n","embedder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7ipU44zg47U"},"outputs":[],"source":["img_random = torch.randn(32, 3, 32, 32)\n","img_emb = embedder(img_random)\n","img_emb.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dml_jyEjg47U"},"outputs":[],"source":["def get_embeded_vector(embedder, dataloader):\n","    vector = []\n","    for i, (images, labels) in enumerate(tqdm(dataloader)):\n","        with torch.no_grad():\n","            v = embedder(images).squeeze().cpu()\n","        vector.extend(v.detach().numpy())\n","\n","    return vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY5GbSPPg47U"},"outputs":[],"source":["if not os.path.exists(path_embeded_train):\n","    train_vectors = get_embeded_vector(embedder, train_dataloader)\n","    print(len(train_vectors), train_vectors[0].shape)\n","    with open(path_embeded_train, mode='wb') as f:\n","        pickle.dump(train_vectors, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7dGilVBg47V"},"outputs":[],"source":["if not os.path.exists(path_embeded_test):\n","    test_vectors = get_embeded_vector(embedder, test_dataloader)\n","    print(len(test_vectors))\n","    with open(path_embeded_test, mode='wb') as f:\n","        pickle.dump(test_vectors, f)"]},{"cell_type":"markdown","metadata":{"id":"BV3BAUhFg47V"},"source":["## Step 5. Faiss indexes\n","\n","検索用インデックスを生成する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgopBTHQg47V"},"outputs":[],"source":["import faiss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75-lUtStg47V"},"outputs":[],"source":["path_indexer = f'{models_path}/{dataset_name}_{encoder_name}_indexer.faiss'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGwKWWDhg47V"},"outputs":[],"source":["def clear_indexer():\n","    if os.path.exists(path_indexer):\n","        os.remove(path_indexer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXGtW9ofg47V"},"outputs":[],"source":["clear_indexer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mpydq0pcg47V"},"outputs":[],"source":["with open(path_embeded_train, mode='rb') as f:\n","    train_vectors = np.array(pickle.load(f))\n","\n","train_vectors.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbBJ3MPsg47V"},"outputs":[],"source":["class FlatIndexer(object):\n","\n","    def __init__(self, vector_sz: int, nlist=10, path=None):\n","        if path and os.path.exists(path):\n","            index_cpu = faiss.read_index(path)\n","            self.indexer = faiss.index_cpu_to_all_gpus(index_cpu)\n","        else:\n","            #index_cpu = faiss.IndexFlatIP(vector_sz) # Not Work\n","            quantizer = faiss.IndexFlatL2(vector_sz)\n","            index_cpu = faiss.IndexIVFFlat(quantizer, vector_sz, nlist, faiss.METRIC_L2)\n","            res = faiss.StandardGpuResources()\n","            self.indexer = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n","\n","    def index_data(self, vectors):\n","        self.indexer.train(vectors)\n","        self.indexer.add(vectors)\n","\n","    def search_knn(self, query_vectors: np.array, top_docs: int):\n","        scores, indexes = self.indexer.search(query_vectors, top_docs)\n","        return scores, indexes\n","\n","    def save_index(self, path):\n","        index_cpu = faiss.index_gpu_to_cpu(self.indexer)\n","        faiss.write_index(index_cpu, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jefQ-hKQg47V"},"outputs":[],"source":["if not os.path.exists(path_indexer):\n","    indexer = FlatIndexer(256, nlist=20)\n","    indexer.index_data(train_vectors)\n","    indexer.save_index(path_indexer)\n","else:\n","    indexer = FlatIndexer(256, path=path_indexer)"]},{"cell_type":"markdown","metadata":{"id":"gTllGdBQg47V"},"source":["## Step 6. Image similarity search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8-BSu_Pg47V"},"outputs":[],"source":["with open(path_embeded_test, mode='rb') as f:\n","    test_vectors = np.array(pickle.load(f))\n","\n","test_vectors.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6a9YKuxg47W"},"outputs":[],"source":["scores, indexes = indexer.search_knn(test_vectors, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVlqjQllg47W"},"outputs":[],"source":["scores.shape, indexes.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jE4IWzHg47W"},"outputs":[],"source":["for i in range(0, 20):\n","    print(i, indexes[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-cyjVdcg47W"},"outputs":[],"source":["scores[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMJiET6Zg47W"},"outputs":[],"source":["def show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name):\n","    \" train_dataset の中から test_dataset で指定された画像に似ている画像を検索する \"\n","    fig, axes = plt.subplots(1, 11, figsize=(15,5))\n","    test_image, test_label = test_dataset[i]\n","    axes[0].imshow(test_image)\n","    axes[0].set_xticks([])\n","    axes[0].set_yticks([])\n","    axes[0].set_title(f'Q[{i}]')\n","\n","    for col, idx in enumerate(indexes[i]):\n","        img, label = train_dataset[idx]\n","        axes[col+1].set_title(f'A[{idx}]')\n","        axes[col+1].imshow(img)\n","        axes[col+1].set_xticks([])\n","        axes[col+1].set_yticks([])\n","    plt.show()\n","    fig.savefig(f'{figs_path}/{dataset_name}_{encoder_name}_search_images_{i}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_q72lLAag47W"},"outputs":[],"source":["for i in range(10):\n","    show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRSE6U29g47W"},"outputs":[],"source":["def merge_search_images(figs_path, dataset_name, encoder_name):\n","    files = glob.glob(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images_*.png\")\n","    images = None\n","    for file in sorted(files):\n","        im = np.array(Image.open(file).convert('RGB'))\n","        h, w, c = im.shape\n","        im = im[120:h-130, 110:w-80, :] # trim\n","        if images is None:\n","            images = im\n","        else:\n","            images = np.append(images, im, axis=0)\n","    img = Image.fromarray(images)\n","    img.save(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images.png\")\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfZpBRa4g47W"},"outputs":[],"source":["img = merge_search_images(figs_path, dataset_name, encoder_name)\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baERYUosg47W"},"outputs":[],"source":["# !nb-clean clean notebook.ipynb"]},{"cell_type":"markdown","metadata":{"id":"b0gW17_sg47X"},"source":["![]()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"cifar10_autoencoder.ipynb","provenance":[],"mount_file_id":"13jzF-wCH0a1rShBp46skYwSk5QfnCYqj","authorship_tag":"ABX9TyNEnAKRbP7w5FljcLPp2nmU"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}