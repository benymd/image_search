{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image similarity search\n",
    "\n",
    "指定された画像と似ている画像を検索する。\n",
    "\n",
    "Step 1. Define AutoEncoder\n",
    "* 画像のencoder として AutoEncoder を定義する。\n",
    "\n",
    "Step 2. Lightning Module\n",
    "* encoder を pytorchlightning 用に定義する。\n",
    "\n",
    "Step 3. Training Encoder\n",
    "* encoder 訓練する。\n",
    "* 訓練済みモデルをファイルに保存する。\n",
    "\n",
    "Step 4. Embedding\n",
    "* 検索対象画像と検索画像を特徴ベクトルへ変換する。\n",
    "* 特徴ベクトルをファイルに保存する。\n",
    "\n",
    "Step 5. Faiss indexes\n",
    "* Step 4 で作成した検索対象画像の特徴ベクトルを検索するためのインデックスを生成する。\n",
    "* インデックスをファイルに保存する。\n",
    "\n",
    "Step 6. Image similarity search\n",
    "* Step 4 で作成した検索画像の特徴ベクトルを使用して、Step 5 のインデックスを検索する。\n",
    "\n",
    "----\n",
    "\n",
    "pytorch:\n",
    "* https://pytorch.org/\n",
    "\n",
    "pytorchligtning:\n",
    "* https://www.pytorchlightning.ai/\n",
    "\n",
    "faiss:\n",
    "* https://github.com/facebookresearch/faiss\n",
    "* https://faiss.ai/\n",
    "\n",
    "STL10:\n",
    "* https://pytorch.org/vision/stable/datasets.html\n",
    "\n",
    "AutoEndoder\n",
    "* https://qiita.com/MuAuan/items/a062d0c245c8f4836399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook runtime\n",
    "import sys\n",
    "\n",
    "runtime = 'local'\n",
    "if 'google.colab' in sys.modules:\n",
    "    runtime = 'colab'\n",
    "elif _dh == ['/kaggle/working']:\n",
    "    runtime = 'kaggle'\n",
    "runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runtime == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runtime == 'colab':\n",
    "    home_path = '/content/drive/MyDrive/image_similarity_search'\n",
    "else:\n",
    "    home_path = '/home/jovyan/image_similarity_search'\n",
    "\n",
    "nbs_path = f'{home_path}/nbs'\n",
    "datasets_path = f'{home_path}/datasets'\n",
    "models_path = f'{home_path}/models'\n",
    "figs_path = f'{home_path}/figs'\n",
    "logs_path = f'{home_path}/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {nbs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import STL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'stl10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_datasets():\n",
    "    path_cifar_10_batches_py = os.path.join(datasets_path, 'stl10_binary')\n",
    "    if os.path.exists(path_cifar_10_batches_py):\n",
    "        shutil.rmtree(path_cifar_10_batches_py)\n",
    "\n",
    "    path_cifar_10_python = os.path.join(datasets_path, 'stl10_binary.tar.gz')\n",
    "    if os.path.exists(path_cifar_10_python):\n",
    "        os.remove(path_cifar_10_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transforms():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        #cifar10_normalization(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str, batch_size: int=32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = int(os.cpu_count() / 2)\n",
    "        self.train_transform = train_transforms()\n",
    "        self.test_transform = test_transforms()\n",
    "        self.classes = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.dims = (3, 96, 96) # channels, width, height\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        STL10(self.data_dir, split='train', download=True)\n",
    "        STL10(self.data_dir, split='test', download=True)\n",
    "\n",
    "    def setup(self, stage=None): #train, val, testデータ分割\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        datasets = STL10(self.data_dir, split='train', transform=self.train_transform)\n",
    "        n_train = int(len(datasets) * 0.8)\n",
    "        n_val = len(datasets) - n_train\n",
    "        self.ds_train, self.ds_val = torch.utils.data.random_split(datasets, [n_train, n_val])\n",
    "        self.ds_test = STL10(self.data_dir, split='test', transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, shuffle=True, drop_last=True, batch_size=self.batch_size, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, shuffle=False, batch_size=self.batch_size, num_workers=0)\n",
    " \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, shuffle=False, batch_size=self.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(datasets_path, BATCH_SIZE)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data size\n",
    "train_dataloader = iter(datamodule.train_dataloader())\n",
    "images, labels = next(train_dataloader)\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Define AutoEncoder\n",
    "\n",
    "画像の特徴ベクトルを抽出するためAutoEncoder を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "encoder(torch.randn(32, 3, 96, 96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 512, out_channels = 256,\n",
    "                                 kernel_size = 2, stride = 2, padding = 0),\n",
    "            nn.ConvTranspose2d(in_channels = 256, out_channels = 16,\n",
    "                                 kernel_size = 2, stride = 2, padding = 0),\n",
    "            nn.ConvTranspose2d(in_channels = 16, out_channels = 3,\n",
    "                                 kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()\n",
    "decoder(torch.randn(32, 512, 12, 12)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = 'autoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        x, y = batch\n",
    "        loss = F.mse_loss(x, self(x))\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "encoder = LitAutoEncoder()\n",
    "encoder = encoder.to(device)  #for gpu\n",
    "summary(encoder.encoder,(3, 96, 96))\n",
    "summary(encoder.decoder,(512, 12, 12))\n",
    "summary(encoder,(3, 96, 96))\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_encoder = f'{models_path}/{dataset_name}_{encoder_name}.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_encoder():\n",
    "    if os.path.exists(path_encoder):\n",
    "        os.remove(path_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor=\"val_loss\")]\n",
    "trainer = pl.Trainer(gpus=1, callbacks=callbacks,\n",
    "                logger=TensorBoardLogger(f'{logs_path}/lightning_logs/', name=f'{dataset_name}_{encoder_name}'))\n",
    "\n",
    "if not os.path.exists(path_encoder):\n",
    "    #trainer = pl.Trainer(max_epochs=10, gpus=1)\n",
    "    trainer.fit(encoder, datamodule)\n",
    "    trainer.save_checkpoint(path_encoder)\n",
    "else:\n",
    "    encoder = encoder.load_from_checkpoint(path_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(device)  #for gpu\n",
    "encoder.freeze()\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# 0.013129102066159248\n",
    "results = trainer.test(encoder, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img, file=None, title=None):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if file:\n",
    "        plt.savefig(file + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original train images\n",
    "dataiter = iter(datamodule.train_dataloader())\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_original', title='train images')\n",
    "print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoded train images\n",
    "images_hat = encoder(images)\n",
    "imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_decoded', title='train decoded images')\n",
    "print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original test images\n",
    "dataiter = iter(datamodule.test_dataloader())\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_original', title='test images')\n",
    "print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_hat = encoder(images)\n",
    "imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_decoded', title='test decoded images')\n",
    "print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Embedding\n",
    "\n",
    "検索対象画像と検索画像を特徴ベクトルへ変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeded_train = f'{models_path}/{dataset_name}_{encoder_name}_embeded_train.pickle'\n",
    "path_embeded_test = f'{models_path}/{dataset_name}_{encoder_name}_embeded_test.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_embedding():\n",
    "    if os.path.exists(path_embeded_train):\n",
    "        os.remove(path_embeded_train)\n",
    "    if os.path.exists(path_embeded_test):\n",
    "        os.remove(path_embeded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = STL10(datasets_path, split='train', download=True)\n",
    "test_dataset = STL10(datasets_path, split='test', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with preprocess\n",
    "train_dataloader = DataLoader(STL10(datasets_path, split='train', download=True, transform=test_transforms()),\n",
    "                               shuffle=False, batch_size=32, num_workers=0)\n",
    "test_dataloader = DataLoader(STL10(datasets_path, split='test', download=True, transform=test_transforms()),\n",
    "                               shuffle=False, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedder_model(autoencoder):\n",
    "    layers = list(autoencoder.encoder.children())\n",
    "    #fv = nn.Sequential(nn.AdaptiveMaxPool2d(output_size=1))\n",
    "    fv = nn.Sequential(nn.Flatten(), nn.Linear(in_features=512*12*12, out_features=512))\n",
    "    model = nn.Sequential(*layers, *fv)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = create_embedder_model(encoder)\n",
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_random = torch.randn(32, 3, 96, 96)\n",
    "img_emb = embedder(img_random)\n",
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeded_vector(embedder, dataloader):\n",
    "    vector = []\n",
    "    for i, (images, labels) in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            v = embedder(images).squeeze().cpu()\n",
    "        vector.extend(v.detach().numpy())\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_embeded_train):\n",
    "    train_vectors = get_embeded_vector(embedder, train_dataloader)\n",
    "    print(len(train_vectors), train_vectors[0].shape)\n",
    "    with open(path_embeded_train, mode='wb') as f:\n",
    "        pickle.dump(train_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_embeded_test):\n",
    "    test_vectors = get_embeded_vector(embedder, test_dataloader)\n",
    "    print(len(test_vectors))\n",
    "    with open(path_embeded_test, mode='wb') as f:\n",
    "        pickle.dump(test_vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Faiss indexes\n",
    "\n",
    "検索用インデックスを生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_indexer = f'{models_path}/{dataset_name}_{encoder_name}_indexer.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_indexer():\n",
    "    if os.path.exists(path_indexer):\n",
    "        os.remove(path_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_indexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_embeded_train, mode='rb') as f:\n",
    "    train_vectors = np.array(pickle.load(f))\n",
    "\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatIndexer(object):\n",
    "\n",
    "    def __init__(self, vector_sz: int, nlist=10, path=None):\n",
    "        if path and os.path.exists(path):\n",
    "            index_cpu = faiss.read_index(path)\n",
    "            self.indexer = faiss.index_cpu_to_all_gpus(index_cpu)\n",
    "        else:\n",
    "            #index_cpu = faiss.IndexFlatIP(vector_sz) # Not Work\n",
    "            quantizer = faiss.IndexFlatL2(vector_sz)\n",
    "            index_cpu = faiss.IndexIVFFlat(quantizer, vector_sz, nlist, faiss.METRIC_L2)\n",
    "            res = faiss.StandardGpuResources()\n",
    "            self.indexer = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n",
    "\n",
    "    def index_data(self, vectors):\n",
    "        self.indexer.train(vectors)\n",
    "        self.indexer.add(vectors)\n",
    "\n",
    "    def search_knn(self, query_vectors: np.array, top_docs: int):\n",
    "        scores, indexes = self.indexer.search(query_vectors, top_docs)\n",
    "        return scores, indexes\n",
    "\n",
    "    def save_index(self, path):\n",
    "        index_cpu = faiss.index_gpu_to_cpu(self.indexer)\n",
    "        faiss.write_index(index_cpu, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_indexer):\n",
    "    indexer = FlatIndexer(512, nlist=20)\n",
    "    indexer.index_data(train_vectors)\n",
    "    indexer.save_index(path_indexer)\n",
    "else:\n",
    "    indexer = FlatIndexer(512, path_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Image similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_embeded_test, mode='rb') as f:\n",
    "    test_vectors = np.array(pickle.load(f))\n",
    "\n",
    "test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indexes = indexer.search_knn(test_vectors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(i, indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name):\n",
    "    \" train_dataset の中から test_dataset で指定された画像に似ている画像を検索する \"\n",
    "    fig, axes = plt.subplots(1, 11, figsize=(15,5))\n",
    "    test_image, test_label = test_dataset[i]\n",
    "    axes[0].imshow(test_image)\n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_yticks([])\n",
    "    axes[0].set_title(f'Q[{i}]')\n",
    "\n",
    "    for col, idx in enumerate(indexes[i]):\n",
    "        img, label = train_dataset[idx]\n",
    "        axes[col+1].set_title(f'A[{idx}]')\n",
    "        axes[col+1].imshow(img)\n",
    "        axes[col+1].set_xticks([])\n",
    "        axes[col+1].set_yticks([])\n",
    "    plt.show()\n",
    "    fig.savefig(f'{figs_path}/{dataset_name}_{encoder_name}_search_images_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_images(figs_path, dataset_name, encoder_name):\n",
    "    files = glob.glob(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images_*.png\")\n",
    "    images = None\n",
    "    for file in sorted(files):\n",
    "        im = np.array(Image.open(file).convert('RGB'))\n",
    "        h, w, c = im.shape\n",
    "        im = im[120:h-130, 110:w-80, :] # trim\n",
    "        if images is None:\n",
    "            images = im\n",
    "        else:\n",
    "            images = np.append(images, im, axis=0)\n",
    "    img = Image.fromarray(images)\n",
    "    img.save(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images.png\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = merge_search_images(figs_path, dataset_name, encoder_name)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nb-clean clean stl10_autoencoder.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZ3x8dThUdC4vaK/PDi+ki",
   "collapsed_sections": [],
   "name": "stl10_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
