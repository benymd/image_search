{"cells":[{"cell_type":"markdown","metadata":{"id":"CGGPguyYDl42"},"source":["# Image search\n","\n","指定された画像と似ている画像を検索する。\n","\n","pytorch:\n","* https://pytorch.org/\n","\n","pytorchligtning:\n","* https://www.pytorchlightning.ai/\n","\n","faiss:\n","* https://github.com/facebookresearch/faiss\n","* https://faiss.ai/\n","\n","Cifar10:\n","* https://pytorch.org/vision/stable/datasets.html\n","\n","AutoEndoder\n","* https://qiita.com/MuAuan/items/a062d0c245c8f4836399"]},{"cell_type":"markdown","metadata":{"id":"yLp_zbjADl47"},"source":["## Setup for Notebook"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3fkjK_VDDl48","executionInfo":{"status":"ok","timestamp":1643637497367,"user_tz":-540,"elapsed":239,"user":{"displayName":"Tsutomu YAMADA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8E4ABJxUciqXWUK5GstytY4Hrhyf2vemsAig5=s64","userId":"06408039033543777891"}},"outputId":"1fb90780-a331-477f-9b10-9dd1f5a14d6b"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'colab'"]},"metadata":{},"execution_count":2}],"source":["# notebook runtime\n","import sys\n","\n","runtime = 'local'\n","if 'google.colab' in sys.modules:\n","    runtime = 'colab'\n","elif _dh == ['/kaggle/working']:\n","    runtime = 'kaggle'\n","runtime"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_j32HcxDl49","executionInfo":{"status":"ok","timestamp":1643637528586,"user_tz":-540,"elapsed":28619,"user":{"displayName":"Tsutomu YAMADA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8E4ABJxUciqXWUK5GstytY4Hrhyf2vemsAig5=s64","userId":"06408039033543777891"}},"outputId":"da74a558-ead6-4fb6-d521-337b5947c66e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if runtime == 'colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sx46rXBSDl4-"},"outputs":[],"source":["if runtime == 'colab':\n","    home_path = '/content/drive/MyDrive/image_similarity_search'\n","else:\n","    home_path = '/home/jovyan/image_similarity_search'\n","\n","nbs_path = f'{home_path}/nbs'\n","datasets_path = f'{home_path}/datasets'\n","models_path = f'{home_path}/models'\n","figs_path = f'{home_path}/figs'\n","logs_path = f'{home_path}/logs'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOeeaEWwDl4-"},"outputs":[],"source":["%cd {nbs_path}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PH-LVaGDl4_"},"outputs":[],"source":["!pip install -q pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smj3pF9NDl5A"},"outputs":[],"source":["!pip install -q faiss-gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrzjXqpTDl5A"},"outputs":[],"source":["!pip install -q nb-clean"]},{"cell_type":"markdown","metadata":{"id":"VS4_tkNKDl5B"},"source":["## Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbkIR6KqDl5B"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","import matplotlib.pyplot as plt\n","import pickle\n","import shutil\n","from PIL import Image\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnBoQ4ogDl5C"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchsummary import summary\n","\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQsVxGzPDl5C"},"outputs":[],"source":["AVAIL_GPUS = min(1, torch.cuda.device_count())\n","BATCH_SIZE = 256 if AVAIL_GPUS else 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d08CBO-6Dl5D"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJeO2P_0Dl5D"},"outputs":[],"source":["import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger"]},{"cell_type":"markdown","metadata":{"id":"99m2Wy_PDl5D"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3HALV_eDl5E"},"outputs":[],"source":["from torchvision.datasets import CIFAR10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uH0hJs5Dl5E"},"outputs":[],"source":["dataset_name = 'cifar10'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UXF2FGQDl5E"},"outputs":[],"source":["def clear_datasets():\n","    path_cifar_10_batches_py = os.path.join(datasets_path, 'cifar-10-batches-py')\n","    if os.path.exists(path_cifar_10_batches_py):\n","        shutil.rmtree(path_cifar_10_batches_py)\n","\n","    path_cifar_10_python = os.path.join(datasets_path, 'cifar-10-python.tar.gz')\n","    if os.path.exists(path_cifar_10_python):\n","        os.remove(path_cifar_10_python)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR1qR2iZDl5E"},"outputs":[],"source":["clear_datasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1k7sFnE3Dl5E"},"outputs":[],"source":["def cifar10_normalization():\n","    normalize = transforms.Normalize(\n","        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n","        std=[x / 255.0 for x in [63.0, 62.1, 66.7]],\n","    )\n","    return normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oGZj3QiDl5F"},"outputs":[],"source":["def train_transforms():\n","    return torchvision.transforms.Compose([\n","        torchvision.transforms.RandomCrop(32, padding=4),\n","        torchvision.transforms.RandomHorizontalFlip(),\n","        torchvision.transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        #cifar10_normalization(),\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTWWfsAMDl5F"},"outputs":[],"source":["def test_transforms():\n","    return torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        #cifar10_normalization(),\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUz2EiykDl5G"},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","\n","    def __init__(self, data_dir: str, batch_size: int=32):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.num_workers = int(os.cpu_count() / 2)\n","        self.train_transform = train_transforms()\n","        self.test_transform = test_transforms()\n","        self.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","        self.num_classes = len(self.classes)\n","        self.dims = (3, 32, 32) # channels, width, height\n","\n","    def prepare_data(self):\n","        # download\n","        CIFAR10(self.data_dir, train=True, download=True)\n","        CIFAR10(self.data_dir, train=False, download=True)\n","\n","    def setup(self, stage=None): #train, val, testデータ分割\n","        # Assign train/val datasets for use in dataloaders\n","        datasets = CIFAR10(self.data_dir, train=True, transform=self.train_transform)\n","        n_train = int(len(datasets) * 0.8)\n","        n_val = len(datasets) - n_train\n","        self.ds_train, self.ds_val = torch.utils.data.random_split(datasets, [n_train, n_val])\n","        self.ds_test = CIFAR10(self.data_dir, train=False, transform=self.test_transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.ds_train, shuffle=True, drop_last=True, batch_size=self.batch_size, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.ds_val, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers)\n"," \n","    def test_dataloader(self):\n","        return DataLoader(self.ds_test, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z8SNsf5Dl5G"},"outputs":[],"source":["datamodule = DataModule(datasets_path, BATCH_SIZE)\n","datamodule.prepare_data()\n","datamodule.setup()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76F1GKZRDl5H"},"outputs":[],"source":["# check data size\n","train_dataloader = iter(datamodule.train_dataloader())\n","images, labels = next(train_dataloader)\n","images.shape, labels.shape"]},{"cell_type":"markdown","metadata":{"id":"Z33YSiT6Dl5H"},"source":["## Training AutoEncoder\n","\n","画像の特徴ベクトルを抽出するためAutoEncoder を学習する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ng19MwjyDl5H"},"outputs":[],"source":["pl.seed_everything(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pabuGE5EDl5H"},"outputs":[],"source":["encoder_name = 'autoencoder'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gKCLiM9Dl5H"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 256, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256)\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AK4-Y03Dl5I"},"outputs":[],"source":["# check data size\n","m = Encoder()\n","m(torch.randn(32, 3, 32, 32)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONv9hK8KDl5I"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=256, out_channels=16,\n","                                 kernel_size=2, stride=2, padding=0),\n","            nn.ConvTranspose2d(in_channels=16, out_channels=3,\n","                                 kernel_size=2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVvUTGuEDl5I"},"outputs":[],"source":["m = Decoder()\n","m(torch.randn(32, 256, 8, 8)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnUr8HNsDl5I"},"outputs":[],"source":["class LitAutoEncoder(pl.LightningModule):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        return x_hat\n","\n","    def training_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"train\")\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"val\")\n","\n","    def test_step(self, batch, batch_idx):\n","        return self._common_step(batch, batch_idx, \"test\")\n","    \n","    def _common_step(self, batch, batch_idx, stage: str):\n","        x, y = batch\n","        loss = F.mse_loss(x, self(x))\n","        self.log(f\"{stage}_loss\", loss, on_step=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aKwdluqDl5Q"},"outputs":[],"source":["# model\n","encoder = LitAutoEncoder()\n","encoder = encoder.to(device)  #for gpu\n","summary(encoder.encoder,(3, 32, 32))\n","summary(encoder.decoder,(256, 8, 8))\n","summary(encoder,(3, 32, 32))\n","print(encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3QpxGSGDl5R"},"outputs":[],"source":["path_encoder = f'{models_path}/{dataset_name}_{encoder_name}.ckpt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSoNLACdDl5R"},"outputs":[],"source":["def clear_encoder():\n","    if os.path.exists(path_encoder):\n","        os.remove(path_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoF4svlHDl5R"},"outputs":[],"source":["clear_encoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEdPL5kTDl5R"},"outputs":[],"source":["callbacks = [EarlyStopping(monitor=\"val_loss\")]\n","trainer = pl.Trainer(gpus=1, callbacks=callbacks, logger=TensorBoardLogger(f'{logs_path}/lightning_logs/', name='autoencoder'))\n","\n","if not os.path.exists(path_encoder):\n","    #trainer = pl.Trainer(max_epochs=10, gpus=1)\n","    trainer.fit(encoder, datamodule)\n","    trainer.save_checkpoint(path_encoder)\n","else:\n","    encoder = encoder.load_from_checkpoint(path_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYjN9tRuDl5R"},"outputs":[],"source":["# functions to show an image\n","def imshow(img, file=None, title=None):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.detach().numpy()\n","    plt.figure(figsize=(20, 10))\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    if title:\n","        plt.title(title)\n","    if file:\n","        plt.savefig(file + '.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heFVGNhjDl5S"},"outputs":[],"source":["# Original train images\n","dataiter = iter(datamodule.train_dataloader())\n","images, labels = dataiter.next()\n","imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_original', title='train images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbwkH5BEDl5S"},"outputs":[],"source":["# Decoded train images\n","images_hat = encoder(images)\n","imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_train_decoded', title='train decoded images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQvIwfiLDl5S"},"outputs":[],"source":["encoder = encoder.to(device)  #for gpu\n","encoder.freeze()\n","encoder.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVCbno4cDl5S"},"outputs":[],"source":["# Test\n","# 'test_loss': 0.003598422510549426\n","results = trainer.test(encoder, datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_YFdAjODl5S"},"outputs":[],"source":["# Original test images\n","dataiter = iter(datamodule.test_dataloader())\n","images, labels = dataiter.next()\n","imshow(torchvision.utils.make_grid(images, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_original', title='test images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFk9qTSQDl5S"},"outputs":[],"source":["images_hat = encoder(images)\n","imshow(torchvision.utils.make_grid(images_hat, nrow=32), f'{figs_path}/{dataset_name}_{encoder_name}_test_decoded', title='test decoded images')\n","print(' '.join('%5s' % datamodule.classes[labels[j]] for j in range(8)))"]},{"cell_type":"markdown","metadata":{"id":"QnqyN07bDl5T"},"source":["## Embedding\n","\n","検索対象画像と検索画像を特徴ベクトルへ変換する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr9t4ffHDl5T"},"outputs":[],"source":["path_embeded_train = f'{models_path}/{dataset_name}_{encoder_name}_embeded_train.pickle'\n","path_embeded_test = f'{models_path}/{dataset_name}_{encoder_name}_embeded_test.pickle'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLfva8lqDl5T"},"outputs":[],"source":["def clear_embedding():\n","    if os.path.exists(path_embeded_train):\n","        os.remove(path_embeded_train)\n","    if os.path.exists(path_embeded_test):\n","        os.remove(path_embeded_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWrHWjNYDl5T"},"outputs":[],"source":["clear_embedding()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8gb1N46Dl5T"},"outputs":[],"source":["train_dataset = CIFAR10(datasets_path, train=True, download=True)\n","test_dataset = CIFAR10(datasets_path, train=False, download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5nhgsOZDl5T"},"outputs":[],"source":["len(train_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJFvvT4CDl5T"},"outputs":[],"source":["# with preprocess\n","train_dataloader = DataLoader(CIFAR10(datasets_path, train=True, download=True, transform=train_transforms()),\n","                               shuffle=False, batch_size=32, num_workers=0)\n","test_dataloader = DataLoader(CIFAR10(datasets_path, train=False, download=True, transform=test_transforms()),\n","                               shuffle=False, batch_size=32, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9EYRroODl5U"},"outputs":[],"source":["len(train_dataloader), len(test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oT7F7RxDl5U"},"outputs":[],"source":["def create_embedder_model(autoencoder):\n","    layers = list(autoencoder.encoder.children())\n","    #fv = nn.Sequential(nn.AdaptiveMaxPool2d(output_size=1))\n","    fv = nn.Sequential(nn.Flatten(), nn.Linear(in_features=256*8*8, out_features=256))\n","    model = nn.Sequential(*layers, *fv)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqdjrxSSDl5U"},"outputs":[],"source":["embedder = create_embedder_model(encoder)\n","embedder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5NlOnGBDl5U"},"outputs":[],"source":["img_random = torch.randn(32, 3, 32, 32)\n","img_emb = embedder(img_random)\n","img_emb.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcvpB4ILDl5U"},"outputs":[],"source":["def get_embeded_vector(embedder, dataloader):\n","    vector = []\n","    for i, (images, labels) in enumerate(tqdm(dataloader)):\n","        with torch.no_grad():\n","            v = embedder(images).squeeze().cpu()\n","        vector.extend(v.detach().numpy())\n","\n","    return vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_WdAuEcDl5U"},"outputs":[],"source":["if not os.path.exists(path_embeded_train):\n","    train_vectors = get_embeded_vector(embedder, train_dataloader)\n","    print(len(train_vectors), train_vectors[0].shape)\n","    with open(path_embeded_train, mode='wb') as f:\n","        pickle.dump(train_vectors, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3nD4eV-Dl5V"},"outputs":[],"source":["if not os.path.exists(path_embeded_test):\n","    test_vectors = get_embeded_vector(embedder, test_dataloader)\n","    print(len(test_vectors))\n","    with open(path_embeded_test, mode='wb') as f:\n","        pickle.dump(test_vectors, f)"]},{"cell_type":"markdown","metadata":{"id":"mUp_K-whDl5V"},"source":["## Faiss\n","\n","検索用インデックスを生成する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oghc0YMLDl5V"},"outputs":[],"source":["import faiss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX4QppbJDl5V"},"outputs":[],"source":["path_indexer = f'{models_path}/{dataset_name}_{encoder_name}_indexer.faiss'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grJWc7RnDl5V"},"outputs":[],"source":["def clear_indexer():\n","    if os.path.exists(path_indexer):\n","        os.remove(path_indexer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQSns3s9Dl5V"},"outputs":[],"source":["clear_indexer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tfKW53TDl5V"},"outputs":[],"source":["with open(path_embeded_train, mode='rb') as f:\n","    train_vectors = np.array(pickle.load(f))\n","\n","train_vectors.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFuaPBvzDl5W"},"outputs":[],"source":["with open(path_embeded_test, mode='rb') as f:\n","    test_vectors = np.array(pickle.load(f))\n","\n","test_vectors.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7t3pw4X3Dl5W"},"outputs":[],"source":["class FlatIndexer(object):\n","\n","    def __init__(self, vector_sz: int, nlist=10, path=None):\n","        if path and os.path.exists(path):\n","            index_cpu = faiss.read_index(path)\n","            self.indexer = faiss.index_cpu_to_all_gpus(index_cpu)\n","        else:\n","            #index_cpu = faiss.IndexFlatIP(vector_sz) # Not Work\n","            quantizer = faiss.IndexFlatL2(vector_sz)\n","            index_cpu = faiss.IndexIVFFlat(quantizer, vector_sz, nlist, faiss.METRIC_L2)\n","            res = faiss.StandardGpuResources()\n","            self.indexer = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n","\n","    def index_data(self, vectors):\n","        self.indexer.train(vectors)\n","        self.indexer.add(vectors)\n","\n","    def search_knn(self, query_vectors: np.array, top_docs: int):\n","        scores, indexes = self.indexer.search(query_vectors, top_docs)\n","        return scores, indexes\n","\n","    def save_index(self, path):\n","        index_cpu = faiss.index_gpu_to_cpu(self.indexer)\n","        faiss.write_index(index_cpu, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy87XWRDDl5W"},"outputs":[],"source":["if not os.path.exists(path_indexer):\n","    indexer = FlatIndexer(256, nlist=20)\n","    indexer.index_data(train_vectors)\n","    indexer.save_index(path_indexer)\n","else:\n","    indexer = FlatIndexer(256, path=path_indexer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7fwB1EpDl5W"},"outputs":[],"source":["scores, indexes = indexer.search_knn(test_vectors, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jo8KMRtODl5W"},"outputs":[],"source":["scores.shape, indexes.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZYLyTIADl5W"},"outputs":[],"source":["for i in range(0, 20):\n","    print(i, indexes[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KavtIbHDl5W"},"outputs":[],"source":["scores[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKfj6CauDl5X"},"outputs":[],"source":["def show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name):\n","    \" train_dataset の中から test_dataset で指定された画像に似ている画像を検索する \"\n","    fig, axes = plt.subplots(1, 11, figsize=(15,5))\n","    test_image, test_label = test_dataset[i]\n","    axes[0].imshow(test_image)\n","    axes[0].set_xticks([])\n","    axes[0].set_yticks([])\n","    axes[0].set_title(f'Q[{i}]')\n","\n","    for col, idx in enumerate(indexes[i]):\n","        img, label = train_dataset[idx]\n","        axes[col+1].set_title(f'A[{idx}]')\n","        axes[col+1].imshow(img)\n","        axes[col+1].set_xticks([])\n","        axes[col+1].set_yticks([])\n","    plt.show()\n","    fig.savefig(f'{figs_path}/{dataset_name}_{encoder_name}_search_images_{i}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gt7JcrySDl5X"},"outputs":[],"source":["for i in range(10):\n","    show_search_images(train_dataset, test_dataset, indexes, i, dataset_name, encoder_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHWNs7Q2Dl5X"},"outputs":[],"source":["def merge_search_images(figs_path, dataset_name, encoder_name):\n","    files = glob.glob(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images_*.png\")\n","    images = None\n","    for file in sorted(files):\n","        im = np.array(Image.open(file).convert('RGB'))\n","        h, w, c = im.shape\n","        im = im[120:h-130, 110:w-80, :] # trim\n","        if images is None:\n","            images = im\n","        else:\n","            images = np.append(images, im, axis=0)\n","    img = Image.fromarray(images)\n","    img.save(f\"{figs_path}/{dataset_name}_{encoder_name}_search_images.png\")\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4axjN3JDl5X"},"outputs":[],"source":["img = merge_search_images(figs_path, dataset_name, encoder_name)\n","img"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cifar10.ipynb","provenance":[],"authorship_tag":"ABX9TyPCLMHDaBp6jT+u8uLkdkf3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}